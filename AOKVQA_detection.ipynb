{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from utils.store_data import ReadData\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.arguments import Arguments\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from linear_probe import get_linear_acc\n",
    "from utils.seed import fix_seed\n",
    "import random\n",
    "import os\n",
    "from answer_judge.vqaEval import VQAEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(0)\n",
    "data_reader=ReadData('/root/autodl-tmp/hallscope/AOKVQA/train')\n",
    "data=data_reader.read_all()\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>most_likely</th>\n",
       "      <th>responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[How many people is the food on the tray meant...</td>\n",
       "      <td>[['two', 'two', 'two', 'two', 'two', 'two', 't...</td>\n",
       "      <td>{'embedding': [[[ 0.00099182  0.0020752  -0.00...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Where is the boy most likely to be headed?\\nA...</td>\n",
       "      <td>[['out', 'sitting room', 'to kitchen', 'to wor...</td>\n",
       "      <td>{'embedding': [[[-0.01141357  0.00665283  0.00...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[What is the usual method to pay for parking h...</td>\n",
       "      <td>[['credit card', 'card', 'card', 'credit card'...</td>\n",
       "      <td>{'embedding': [[[ 0.01519775  0.00805664 -0.01...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[How many people wearing tan pants and black s...</td>\n",
       "      <td>[['one', 'one', 'five', 'one', 'four', 'five',...</td>\n",
       "      <td>{'embedding': [[[ 0.00099182  0.0020752  -0.00...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Why can't people go down this road at this ti...</td>\n",
       "      <td>[['barrier', \"it's blocked\", 'accident', 'bus ...</td>\n",
       "      <td>{'embedding': [[[-0.0402832   0.02270508  0.01...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  [How many people is the food on the tray meant...   \n",
       "1  [Where is the boy most likely to be headed?\\nA...   \n",
       "2  [What is the usual method to pay for parking h...   \n",
       "3  [How many people wearing tan pants and black s...   \n",
       "4  [Why can't people go down this road at this ti...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  [['two', 'two', 'two', 'two', 'two', 'two', 't...   \n",
       "1  [['out', 'sitting room', 'to kitchen', 'to wor...   \n",
       "2  [['credit card', 'card', 'card', 'credit card'...   \n",
       "3  [['one', 'one', 'five', 'one', 'four', 'five',...   \n",
       "4  [['barrier', \"it's blocked\", 'accident', 'bus ...   \n",
       "\n",
       "                                         most_likely responses  \n",
       "0  {'embedding': [[[ 0.00099182  0.0020752  -0.00...      None  \n",
       "1  {'embedding': [[[-0.01141357  0.00665283  0.00...      None  \n",
       "2  {'embedding': [[[ 0.01519775  0.00805664 -0.01...      None  \n",
       "3  {'embedding': [[[ 0.00099182  0.0020752  -0.00...      None  \n",
       "4  {'embedding': [[[-0.0402832   0.02270508  0.01...      None  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3129, 4)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rock']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[i]['most_likely']['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the white object in the ground in front of the animals?\\nAnswer the question using a single word or phrase.\\n']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[i]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['gravel', 'rocks', 'rocks', 'rock', 'boulder', 'stone', 'concrete', 'rock', 'stone', 'stone']\"]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[i]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=df['most_likely'].apply(lambda x: x['embedding']).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=np.concatenate(embeddings,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3129, 29, 3584)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_lines(x):\n",
    "    x=ast.literal_eval(x[0])\n",
    "    return x\n",
    "\n",
    "df['answer']=df['answer'].apply(transpose_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [two, two, two, two, two, two, two, two, one, ...\n",
       "1       [out, sitting room, to kitchen, to work, other...\n",
       "2       [credit card, card, card, credit card, credit ...\n",
       "3       [one, one, five, one, four, five, one, one, on...\n",
       "4       [barrier, it's blocked, accident, bus accident...\n",
       "                              ...                        \n",
       "3124    [information, telephone, information, informat...\n",
       "3125    [protection, riding, protection, plastic, plas...\n",
       "3126    [on right, lady, middle woman, pale woman, blo...\n",
       "3127    [nirates, mirates, dubai, emirates, dubai, sau...\n",
       "3128    [fire alarm, smoke detector, smoke detector, s...\n",
       "Name: answer, Length: 3129, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label=df['answer'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['two', 'two', 'two', 'two', 'two', 'two', 'two', 'two', 'one', 'two'],\n",
       " ['out',\n",
       "  'sitting room',\n",
       "  'to kitchen',\n",
       "  'to work',\n",
       "  'other room',\n",
       "  'living room',\n",
       "  'prom',\n",
       "  'living room',\n",
       "  'work',\n",
       "  'work']]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_label[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses=df['most_likely'].apply(lambda x: x['response']).to_list()\n",
    "rsps=[]\n",
    "for x in responses:\n",
    "    rsps+=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'bathroom', 'Credit Card', '4', 'accident']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsps[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3129 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3129/3129 [00:01<00:00, 2834.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done computing accuracy\n",
      "{0: 33.33, 1: 0.0, 2: 100, 3: 33.33, 4: 66.67, 5: 100, 6: 66.67, 7: 66.67, 8: 100, 9: 0.0, 10: 100, 11: 100, 12: 100, 13: 66.67, 14: 33.33, 15: 0.0, 16: 0.0, 17: 100, 18: 66.67, 19: 100, 20: 100, 21: 100, 22: 100, 23: 100, 24: 33.33, 25: 33.33, 26: 33.33, 27: 33.33, 28: 0.0, 29: 100, 30: 33.33, 31: 100, 32: 100, 33: 100, 34: 0.0, 35: 0.0, 36: 100, 37: 66.67, 38: 100, 39: 0.0, 40: 100, 41: 100, 42: 100, 43: 100, 44: 100, 45: 66.67, 46: 100, 47: 0.0, 48: 100, 49: 100, 50: 100, 51: 0.0, 52: 100, 53: 100, 54: 100, 55: 100, 56: 0.0, 57: 33.33, 58: 100, 59: 100, 60: 100, 61: 100, 62: 0.0, 63: 100, 64: 100, 65: 100, 66: 100, 67: 0.0, 68: 0.0, 69: 0.0, 70: 0.0, 71: 33.33, 72: 0.0, 73: 100, 74: 100, 75: 100, 76: 100, 77: 100, 78: 33.33, 79: 100, 80: 0.0, 81: 100, 82: 100, 83: 33.33, 84: 100, 85: 100, 86: 100, 87: 0.0, 88: 0.0, 89: 100, 90: 100, 91: 66.67, 92: 0.0, 93: 0.0, 94: 100, 95: 100, 96: 100, 97: 100, 98: 0.0, 99: 66.67, 100: 100, 101: 100, 102: 0.0, 103: 33.33, 104: 100, 105: 100, 106: 0.0, 107: 100, 108: 0.0, 109: 33.33, 110: 100, 111: 100, 112: 100, 113: 100, 114: 0.0, 115: 100, 116: 100, 117: 100, 118: 0.0, 119: 100, 120: 0.0, 121: 100, 122: 0.0, 123: 0.0, 124: 33.33, 125: 33.33, 126: 100, 127: 100, 128: 100, 129: 0.0, 130: 0.0, 131: 0.0, 132: 0.0, 133: 33.33, 134: 0.0, 135: 100, 136: 0.0, 137: 100, 138: 100, 139: 33.33, 140: 100, 141: 0.0, 142: 100, 143: 66.67, 144: 0.0, 145: 100, 146: 100, 147: 100, 148: 66.67, 149: 33.33, 150: 0.0, 151: 0.0, 152: 100, 153: 0.0, 154: 33.33, 155: 0.0, 156: 100, 157: 0.0, 158: 100, 159: 33.33, 160: 100, 161: 0.0, 162: 0.0, 163: 0.0, 164: 100, 165: 0.0, 166: 100, 167: 100, 168: 100, 169: 100, 170: 100, 171: 0.0, 172: 33.33, 173: 0.0, 174: 100, 175: 33.33, 176: 33.33, 177: 0.0, 178: 100, 179: 0.0, 180: 0.0, 181: 100, 182: 0.0, 183: 0.0, 184: 100, 185: 66.67, 186: 100, 187: 100, 188: 66.67, 189: 100, 190: 66.67, 191: 100, 192: 100, 193: 100, 194: 100, 195: 100, 196: 100, 197: 66.67, 198: 100, 199: 0.0, 200: 66.67, 201: 100, 202: 0.0, 203: 0.0, 204: 100, 205: 0.0, 206: 0.0, 207: 0.0, 208: 0.0, 209: 33.33, 210: 0.0, 211: 100, 212: 100, 213: 0.0, 214: 100, 215: 100, 216: 0.0, 217: 100, 218: 66.67, 219: 0.0, 220: 100, 221: 100, 222: 0.0, 223: 0.0, 224: 0.0, 225: 0.0, 226: 100, 227: 0.0, 228: 0.0, 229: 0.0, 230: 0.0, 231: 100, 232: 100, 233: 0.0, 234: 66.67, 235: 100, 236: 100, 237: 33.33, 238: 100, 239: 0.0, 240: 100, 241: 100, 242: 100, 243: 0.0, 244: 0.0, 245: 100, 246: 0.0, 247: 33.33, 248: 33.33, 249: 100, 250: 100, 251: 33.33, 252: 0.0, 253: 0.0, 254: 100, 255: 100, 256: 100, 257: 0.0, 258: 33.33, 259: 100, 260: 33.33, 261: 100, 262: 100, 263: 100, 264: 33.33, 265: 0.0, 266: 100, 267: 33.33, 268: 33.33, 269: 33.33, 270: 0.0, 271: 0.0, 272: 100, 273: 0.0, 274: 0.0, 275: 100, 276: 0.0, 277: 100, 278: 0.0, 279: 0.0, 280: 66.67, 281: 100, 282: 0.0, 283: 100, 284: 100, 285: 0.0, 286: 100, 287: 100, 288: 100, 289: 100, 290: 0.0, 291: 100, 292: 0.0, 293: 100, 294: 0.0, 295: 0.0, 296: 0.0, 297: 100, 298: 0.0, 299: 100, 300: 66.67, 301: 100, 302: 0.0, 303: 0.0, 304: 0.0, 305: 100, 306: 100, 307: 66.67, 308: 100, 309: 0.0, 310: 66.67, 311: 100, 312: 100, 313: 0.0, 314: 0.0, 315: 0.0, 316: 100, 317: 0.0, 318: 33.33, 319: 66.67, 320: 33.33, 321: 100, 322: 66.67, 323: 0.0, 324: 33.33, 325: 100, 326: 100, 327: 0.0, 328: 66.67, 329: 0.0, 330: 100, 331: 0.0, 332: 100, 333: 100, 334: 0.0, 335: 0.0, 336: 0.0, 337: 100, 338: 100, 339: 100, 340: 100, 341: 100, 342: 0.0, 343: 100, 344: 100, 345: 0.0, 346: 0.0, 347: 66.67, 348: 100, 349: 100, 350: 100, 351: 100, 352: 0.0, 353: 0.0, 354: 0.0, 355: 100, 356: 33.33, 357: 33.33, 358: 100, 359: 0.0, 360: 0.0, 361: 33.33, 362: 0.0, 363: 100, 364: 100, 365: 33.33, 366: 33.33, 367: 100, 368: 0.0, 369: 33.33, 370: 33.33, 371: 100, 372: 33.33, 373: 100, 374: 100, 375: 100, 376: 0.0, 377: 100, 378: 66.67, 379: 0.0, 380: 0.0, 381: 0.0, 382: 100, 383: 100, 384: 100, 385: 100, 386: 66.67, 387: 0.0, 388: 100, 389: 0.0, 390: 100, 391: 0.0, 392: 0.0, 393: 100, 394: 100, 395: 66.67, 396: 0.0, 397: 100, 398: 0.0, 399: 33.33, 400: 33.33, 401: 100, 402: 0.0, 403: 100, 404: 100, 405: 0.0, 406: 100, 407: 33.33, 408: 100, 409: 0.0, 410: 100, 411: 33.33, 412: 100, 413: 0.0, 414: 100, 415: 100, 416: 100, 417: 100, 418: 100, 419: 100, 420: 33.33, 421: 66.67, 422: 0.0, 423: 100, 424: 100, 425: 100, 426: 0.0, 427: 33.33, 428: 0.0, 429: 100, 430: 33.33, 431: 66.67, 432: 100, 433: 100, 434: 33.33, 435: 100, 436: 100, 437: 0.0, 438: 100, 439: 100, 440: 0.0, 441: 100, 442: 0.0, 443: 33.33, 444: 0.0, 445: 100, 446: 33.33, 447: 100, 448: 100, 449: 0.0, 450: 0.0, 451: 66.67, 452: 100, 453: 0.0, 454: 100, 455: 100, 456: 100, 457: 100, 458: 0.0, 459: 100, 460: 100, 461: 0.0, 462: 100, 463: 0.0, 464: 0.0, 465: 0.0, 466: 0.0, 467: 100, 468: 100, 469: 0.0, 470: 0.0, 471: 100, 472: 100, 473: 100, 474: 100, 475: 100, 476: 66.67, 477: 100, 478: 100, 479: 0.0, 480: 0.0, 481: 100, 482: 100, 483: 100, 484: 0.0, 485: 100, 486: 0.0, 487: 100, 488: 0.0, 489: 0.0, 490: 0.0, 491: 0.0, 492: 0.0, 493: 100, 494: 100, 495: 0.0, 496: 100, 497: 100, 498: 0.0, 499: 33.33, 500: 0.0, 501: 0.0, 502: 100, 503: 100, 504: 100, 505: 100, 506: 0.0, 507: 0.0, 508: 100, 509: 100, 510: 100, 511: 100, 512: 0.0, 513: 0.0, 514: 100, 515: 33.33, 516: 33.33, 517: 0.0, 518: 0.0, 519: 66.67, 520: 0.0, 521: 66.67, 522: 0.0, 523: 100, 524: 100, 525: 100, 526: 66.67, 527: 33.33, 528: 100, 529: 100, 530: 100, 531: 100, 532: 66.67, 533: 100, 534: 100, 535: 0.0, 536: 100, 537: 0.0, 538: 0.0, 539: 100, 540: 100, 541: 66.67, 542: 100, 543: 33.33, 544: 0.0, 545: 0.0, 546: 33.33, 547: 0.0, 548: 100, 549: 0.0, 550: 100, 551: 100, 552: 100, 553: 0.0, 554: 33.33, 555: 100, 556: 0.0, 557: 0.0, 558: 100, 559: 0.0, 560: 0.0, 561: 100, 562: 0.0, 563: 33.33, 564: 100, 565: 0.0, 566: 0.0, 567: 100, 568: 66.67, 569: 0.0, 570: 66.67, 571: 0.0, 572: 0.0, 573: 100, 574: 0.0, 575: 100, 576: 33.33, 577: 66.67, 578: 33.33, 579: 0.0, 580: 0.0, 581: 0.0, 582: 66.67, 583: 66.67, 584: 0.0, 585: 100, 586: 100, 587: 0.0, 588: 0.0, 589: 100, 590: 0.0, 591: 33.33, 592: 0.0, 593: 0.0, 594: 0.0, 595: 0.0, 596: 100, 597: 0.0, 598: 0.0, 599: 33.33, 600: 100, 601: 66.67, 602: 100, 603: 100, 604: 66.67, 605: 100, 606: 66.67, 607: 100, 608: 0.0, 609: 0.0, 610: 0.0, 611: 100, 612: 100, 613: 0.0, 614: 0.0, 615: 0.0, 616: 0.0, 617: 0.0, 618: 0.0, 619: 66.67, 620: 0.0, 621: 0.0, 622: 100, 623: 100, 624: 0.0, 625: 0.0, 626: 100, 627: 33.33, 628: 100, 629: 100, 630: 100, 631: 100, 632: 66.67, 633: 100, 634: 100, 635: 0.0, 636: 100, 637: 0.0, 638: 66.67, 639: 66.67, 640: 33.33, 641: 100, 642: 100, 643: 33.33, 644: 0.0, 645: 100, 646: 100, 647: 0.0, 648: 0.0, 649: 100, 650: 100, 651: 0.0, 652: 100, 653: 66.67, 654: 0.0, 655: 100, 656: 0.0, 657: 0.0, 658: 100, 659: 100, 660: 100, 661: 100, 662: 33.33, 663: 100, 664: 0.0, 665: 100, 666: 100, 667: 100, 668: 0.0, 669: 100, 670: 100, 671: 33.33, 672: 33.33, 673: 100, 674: 33.33, 675: 33.33, 676: 0.0, 677: 0.0, 678: 100, 679: 100, 680: 0.0, 681: 0.0, 682: 0.0, 683: 0.0, 684: 100, 685: 100, 686: 0.0, 687: 0.0, 688: 66.67, 689: 66.67, 690: 0.0, 691: 0.0, 692: 100, 693: 0.0, 694: 0.0, 695: 0.0, 696: 33.33, 697: 100, 698: 0.0, 699: 33.33, 700: 0.0, 701: 0.0, 702: 100, 703: 0.0, 704: 100, 705: 0.0, 706: 33.33, 707: 100, 708: 0.0, 709: 33.33, 710: 0.0, 711: 100, 712: 100, 713: 0.0, 714: 66.67, 715: 33.33, 716: 100, 717: 100, 718: 100, 719: 100, 720: 100, 721: 100, 722: 0.0, 723: 0.0, 724: 100, 725: 33.33, 726: 100, 727: 100, 728: 100, 729: 100, 730: 33.33, 731: 100, 732: 100, 733: 100, 734: 0.0, 735: 0.0, 736: 0.0, 737: 100, 738: 100, 739: 0.0, 740: 33.33, 741: 100, 742: 33.33, 743: 100, 744: 0.0, 745: 0.0, 746: 100, 747: 100, 748: 33.33, 749: 100, 750: 0.0, 751: 33.33, 752: 100, 753: 0.0, 754: 33.33, 755: 33.33, 756: 100, 757: 0.0, 758: 0.0, 759: 100, 760: 100, 761: 100, 762: 100, 763: 100, 764: 100, 765: 33.33, 766: 0.0, 767: 100, 768: 100, 769: 0.0, 770: 100, 771: 33.33, 772: 0.0, 773: 0.0, 774: 100, 775: 0.0, 776: 100, 777: 0.0, 778: 100, 779: 0.0, 780: 100, 781: 100, 782: 100, 783: 0.0, 784: 100, 785: 100, 786: 100, 787: 100, 788: 33.33, 789: 33.33, 790: 0.0, 791: 33.33, 792: 100, 793: 100, 794: 0.0, 795: 0.0, 796: 100, 797: 0.0, 798: 100, 799: 100, 800: 0.0, 801: 100, 802: 0.0, 803: 0.0, 804: 100, 805: 100, 806: 100, 807: 66.67, 808: 0.0, 809: 100, 810: 33.33, 811: 33.33, 812: 0.0, 813: 0.0, 814: 100, 815: 100, 816: 0.0, 817: 0.0, 818: 100, 819: 33.33, 820: 0.0, 821: 0.0, 822: 100, 823: 33.33, 824: 0.0, 825: 0.0, 826: 100, 827: 100, 828: 33.33, 829: 33.33, 830: 0.0, 831: 66.67, 832: 100, 833: 33.33, 834: 100, 835: 0.0, 836: 66.67, 837: 100, 838: 0.0, 839: 0.0, 840: 100, 841: 100, 842: 100, 843: 100, 844: 66.67, 845: 33.33, 846: 100, 847: 100, 848: 33.33, 849: 66.67, 850: 100, 851: 100, 852: 100, 853: 100, 854: 33.33, 855: 100, 856: 66.67, 857: 0.0, 858: 0.0, 859: 0.0, 860: 0.0, 861: 33.33, 862: 0.0, 863: 100, 864: 0.0, 865: 0.0, 866: 33.33, 867: 33.33, 868: 66.67, 869: 100, 870: 33.33, 871: 0.0, 872: 100, 873: 100, 874: 100, 875: 33.33, 876: 33.33, 877: 100, 878: 33.33, 879: 66.67, 880: 100, 881: 100, 882: 33.33, 883: 100, 884: 100, 885: 33.33, 886: 0.0, 887: 0.0, 888: 100, 889: 66.67, 890: 0.0, 891: 0.0, 892: 100, 893: 100, 894: 33.33, 895: 100, 896: 100, 897: 100, 898: 0.0, 899: 33.33, 900: 100, 901: 100, 902: 66.67, 903: 100, 904: 100, 905: 100, 906: 0.0, 907: 0.0, 908: 100, 909: 0.0, 910: 0.0, 911: 100, 912: 100, 913: 0.0, 914: 0.0, 915: 100, 916: 0.0, 917: 100, 918: 100, 919: 0.0, 920: 0.0, 921: 66.67, 922: 33.33, 923: 0.0, 924: 100, 925: 0.0, 926: 100, 927: 0.0, 928: 0.0, 929: 66.67, 930: 0.0, 931: 0.0, 932: 0.0, 933: 100, 934: 100, 935: 100, 936: 33.33, 937: 66.67, 938: 0.0, 939: 100, 940: 100, 941: 33.33, 942: 100, 943: 66.67, 944: 100, 945: 100, 946: 100, 947: 100, 948: 0.0, 949: 100, 950: 100, 951: 100, 952: 100, 953: 100, 954: 100, 955: 0.0, 956: 0.0, 957: 100, 958: 100, 959: 66.67, 960: 0.0, 961: 100, 962: 33.33, 963: 100, 964: 0.0, 965: 33.33, 966: 100, 967: 100, 968: 0.0, 969: 0.0, 970: 100, 971: 100, 972: 0.0, 973: 100, 974: 0.0, 975: 66.67, 976: 100, 977: 100, 978: 0.0, 979: 33.33, 980: 33.33, 981: 100, 982: 0.0, 983: 0.0, 984: 100, 985: 0.0, 986: 0.0, 987: 100, 988: 0.0, 989: 0.0, 990: 0.0, 991: 100, 992: 100, 993: 100, 994: 33.33, 995: 100, 996: 0.0, 997: 0.0, 998: 0.0, 999: 100, 1000: 0.0, 1001: 0.0, 1002: 0.0, 1003: 100, 1004: 100, 1005: 100, 1006: 100, 1007: 0.0, 1008: 0.0, 1009: 100, 1010: 100, 1011: 100, 1012: 100, 1013: 66.67, 1014: 0.0, 1015: 0.0, 1016: 100, 1017: 100, 1018: 0.0, 1019: 0.0, 1020: 66.67, 1021: 0.0, 1022: 0.0, 1023: 66.67, 1024: 0.0, 1025: 100, 1026: 66.67, 1027: 0.0, 1028: 100, 1029: 0.0, 1030: 33.33, 1031: 100, 1032: 0.0, 1033: 100, 1034: 100, 1035: 0.0, 1036: 100, 1037: 0.0, 1038: 100, 1039: 0.0, 1040: 0.0, 1041: 100, 1042: 0.0, 1043: 0.0, 1044: 66.67, 1045: 0.0, 1046: 33.33, 1047: 100, 1048: 66.67, 1049: 100, 1050: 100, 1051: 0.0, 1052: 100, 1053: 0.0, 1054: 33.33, 1055: 33.33, 1056: 33.33, 1057: 33.33, 1058: 0.0, 1059: 100, 1060: 100, 1061: 100, 1062: 100, 1063: 100, 1064: 0.0, 1065: 100, 1066: 0.0, 1067: 100, 1068: 0.0, 1069: 0.0, 1070: 33.33, 1071: 0.0, 1072: 100, 1073: 100, 1074: 100, 1075: 66.67, 1076: 100, 1077: 100, 1078: 66.67, 1079: 100, 1080: 0.0, 1081: 100, 1082: 0.0, 1083: 100, 1084: 100, 1085: 100, 1086: 0.0, 1087: 100, 1088: 0.0, 1089: 66.67, 1090: 100, 1091: 100, 1092: 66.67, 1093: 100, 1094: 0.0, 1095: 0.0, 1096: 66.67, 1097: 0.0, 1098: 100, 1099: 100, 1100: 100, 1101: 0.0, 1102: 100, 1103: 0.0, 1104: 0.0, 1105: 100, 1106: 100, 1107: 100, 1108: 0.0, 1109: 0.0, 1110: 0.0, 1111: 100, 1112: 100, 1113: 0.0, 1114: 33.33, 1115: 100, 1116: 100, 1117: 100, 1118: 100, 1119: 0.0, 1120: 100, 1121: 0.0, 1122: 0.0, 1123: 0.0, 1124: 100, 1125: 66.67, 1126: 0.0, 1127: 33.33, 1128: 0.0, 1129: 100, 1130: 100, 1131: 66.67, 1132: 100, 1133: 100, 1134: 100, 1135: 0.0, 1136: 100, 1137: 100, 1138: 100, 1139: 100, 1140: 100, 1141: 33.33, 1142: 0.0, 1143: 0.0, 1144: 100, 1145: 33.33, 1146: 0.0, 1147: 100, 1148: 0.0, 1149: 33.33, 1150: 33.33, 1151: 100, 1152: 0.0, 1153: 0.0, 1154: 100, 1155: 100, 1156: 100, 1157: 100, 1158: 33.33, 1159: 33.33, 1160: 0.0, 1161: 33.33, 1162: 100, 1163: 66.67, 1164: 0.0, 1165: 0.0, 1166: 100, 1167: 100, 1168: 100, 1169: 100, 1170: 100, 1171: 66.67, 1172: 100, 1173: 0.0, 1174: 33.33, 1175: 100, 1176: 100, 1177: 100, 1178: 0.0, 1179: 100, 1180: 100, 1181: 66.67, 1182: 0.0, 1183: 100, 1184: 0.0, 1185: 66.67, 1186: 0.0, 1187: 66.67, 1188: 0.0, 1189: 66.67, 1190: 0.0, 1191: 33.33, 1192: 33.33, 1193: 0.0, 1194: 100, 1195: 33.33, 1196: 100, 1197: 0.0, 1198: 100, 1199: 0.0, 1200: 100, 1201: 100, 1202: 0.0, 1203: 100, 1204: 100, 1205: 100, 1206: 66.67, 1207: 33.33, 1208: 100, 1209: 0.0, 1210: 100, 1211: 100, 1212: 100, 1213: 100, 1214: 100, 1215: 100, 1216: 100, 1217: 100, 1218: 0.0, 1219: 100, 1220: 0.0, 1221: 0.0, 1222: 0.0, 1223: 33.33, 1224: 0.0, 1225: 100, 1226: 33.33, 1227: 100, 1228: 33.33, 1229: 0.0, 1230: 0.0, 1231: 33.33, 1232: 66.67, 1233: 100, 1234: 100, 1235: 100, 1236: 66.67, 1237: 100, 1238: 66.67, 1239: 0.0, 1240: 100, 1241: 100, 1242: 100, 1243: 100, 1244: 0.0, 1245: 66.67, 1246: 66.67, 1247: 0.0, 1248: 0.0, 1249: 100, 1250: 0.0, 1251: 0.0, 1252: 100, 1253: 0.0, 1254: 0.0, 1255: 100, 1256: 0.0, 1257: 0.0, 1258: 100, 1259: 0.0, 1260: 0.0, 1261: 100, 1262: 100, 1263: 0.0, 1264: 100, 1265: 66.67, 1266: 100, 1267: 0.0, 1268: 100, 1269: 0.0, 1270: 100, 1271: 100, 1272: 100, 1273: 66.67, 1274: 100, 1275: 100, 1276: 0.0, 1277: 100, 1278: 100, 1279: 100, 1280: 100, 1281: 0.0, 1282: 100, 1283: 0.0, 1284: 0.0, 1285: 0.0, 1286: 100, 1287: 0.0, 1288: 0.0, 1289: 66.67, 1290: 100, 1291: 0.0, 1292: 0.0, 1293: 33.33, 1294: 0.0, 1295: 100, 1296: 100, 1297: 0.0, 1298: 100, 1299: 100, 1300: 0.0, 1301: 33.33, 1302: 100, 1303: 100, 1304: 0.0, 1305: 100, 1306: 100, 1307: 100, 1308: 100, 1309: 0.0, 1310: 100, 1311: 0.0, 1312: 100, 1313: 100, 1314: 100, 1315: 100, 1316: 0.0, 1317: 100, 1318: 100, 1319: 100, 1320: 66.67, 1321: 0.0, 1322: 0.0, 1323: 0.0, 1324: 0.0, 1325: 0.0, 1326: 0.0, 1327: 0.0, 1328: 66.67, 1329: 100, 1330: 100, 1331: 100, 1332: 0.0, 1333: 0.0, 1334: 0.0, 1335: 100, 1336: 0.0, 1337: 100, 1338: 0.0, 1339: 100, 1340: 0.0, 1341: 33.33, 1342: 0.0, 1343: 66.67, 1344: 100, 1345: 0.0, 1346: 0.0, 1347: 100, 1348: 100, 1349: 100, 1350: 33.33, 1351: 100, 1352: 0.0, 1353: 100, 1354: 33.33, 1355: 0.0, 1356: 0.0, 1357: 100, 1358: 0.0, 1359: 0.0, 1360: 0.0, 1361: 0.0, 1362: 100, 1363: 100, 1364: 100, 1365: 0.0, 1366: 66.67, 1367: 100, 1368: 100, 1369: 0.0, 1370: 100, 1371: 0.0, 1372: 100, 1373: 0.0, 1374: 0.0, 1375: 100, 1376: 0.0, 1377: 100, 1378: 0.0, 1379: 100, 1380: 100, 1381: 100, 1382: 100, 1383: 100, 1384: 100, 1385: 0.0, 1386: 100, 1387: 100, 1388: 0.0, 1389: 100, 1390: 0.0, 1391: 100, 1392: 33.33, 1393: 0.0, 1394: 100, 1395: 33.33, 1396: 0.0, 1397: 100, 1398: 100, 1399: 100, 1400: 0.0, 1401: 66.67, 1402: 0.0, 1403: 100, 1404: 0.0, 1405: 0.0, 1406: 100, 1407: 0.0, 1408: 0.0, 1409: 100, 1410: 100, 1411: 100, 1412: 33.33, 1413: 0.0, 1414: 100, 1415: 0.0, 1416: 100, 1417: 0.0, 1418: 100, 1419: 0.0, 1420: 0.0, 1421: 0.0, 1422: 33.33, 1423: 0.0, 1424: 100, 1425: 66.67, 1426: 100, 1427: 33.33, 1428: 0.0, 1429: 0.0, 1430: 66.67, 1431: 100, 1432: 0.0, 1433: 100, 1434: 66.67, 1435: 100, 1436: 0.0, 1437: 0.0, 1438: 100, 1439: 0.0, 1440: 0.0, 1441: 0.0, 1442: 0.0, 1443: 100, 1444: 33.33, 1445: 100, 1446: 0.0, 1447: 0.0, 1448: 66.67, 1449: 0.0, 1450: 100, 1451: 66.67, 1452: 100, 1453: 0.0, 1454: 100, 1455: 100, 1456: 0.0, 1457: 100, 1458: 33.33, 1459: 100, 1460: 100, 1461: 0.0, 1462: 66.67, 1463: 100, 1464: 100, 1465: 0.0, 1466: 0.0, 1467: 100, 1468: 100, 1469: 33.33, 1470: 0.0, 1471: 100, 1472: 100, 1473: 0.0, 1474: 0.0, 1475: 0.0, 1476: 100, 1477: 100, 1478: 0.0, 1479: 0.0, 1480: 0.0, 1481: 100, 1482: 100, 1483: 0.0, 1484: 66.67, 1485: 100, 1486: 100, 1487: 0.0, 1488: 0.0, 1489: 0.0, 1490: 33.33, 1491: 100, 1492: 100, 1493: 100, 1494: 0.0, 1495: 33.33, 1496: 0.0, 1497: 100, 1498: 66.67, 1499: 100, 1500: 0.0, 1501: 66.67, 1502: 0.0, 1503: 33.33, 1504: 100, 1505: 0.0, 1506: 100, 1507: 0.0, 1508: 66.67, 1509: 0.0, 1510: 0.0, 1511: 33.33, 1512: 0.0, 1513: 100, 1514: 100, 1515: 100, 1516: 0.0, 1517: 0.0, 1518: 0.0, 1519: 66.67, 1520: 0.0, 1521: 33.33, 1522: 100, 1523: 100, 1524: 0.0, 1525: 0.0, 1526: 100, 1527: 0.0, 1528: 66.67, 1529: 100, 1530: 0.0, 1531: 100, 1532: 100, 1533: 100, 1534: 0.0, 1535: 0.0, 1536: 100, 1537: 0.0, 1538: 66.67, 1539: 0.0, 1540: 100, 1541: 33.33, 1542: 33.33, 1543: 100, 1544: 0.0, 1545: 66.67, 1546: 0.0, 1547: 0.0, 1548: 0.0, 1549: 66.67, 1550: 100, 1551: 0.0, 1552: 100, 1553: 66.67, 1554: 100, 1555: 0.0, 1556: 0.0, 1557: 100, 1558: 0.0, 1559: 66.67, 1560: 100, 1561: 0.0, 1562: 100, 1563: 100, 1564: 100, 1565: 100, 1566: 100, 1567: 66.67, 1568: 0.0, 1569: 100, 1570: 66.67, 1571: 0.0, 1572: 0.0, 1573: 0.0, 1574: 0.0, 1575: 0.0, 1576: 100, 1577: 0.0, 1578: 0.0, 1579: 100, 1580: 0.0, 1581: 0.0, 1582: 0.0, 1583: 100, 1584: 0.0, 1585: 66.67, 1586: 0.0, 1587: 100, 1588: 100, 1589: 100, 1590: 100, 1591: 100, 1592: 100, 1593: 100, 1594: 0.0, 1595: 66.67, 1596: 33.33, 1597: 100, 1598: 66.67, 1599: 100, 1600: 100, 1601: 0.0, 1602: 0.0, 1603: 0.0, 1604: 33.33, 1605: 100, 1606: 100, 1607: 0.0, 1608: 0.0, 1609: 100, 1610: 0.0, 1611: 66.67, 1612: 33.33, 1613: 66.67, 1614: 0.0, 1615: 33.33, 1616: 0.0, 1617: 100, 1618: 100, 1619: 100, 1620: 66.67, 1621: 100, 1622: 100, 1623: 66.67, 1624: 66.67, 1625: 100, 1626: 100, 1627: 0.0, 1628: 33.33, 1629: 0.0, 1630: 100, 1631: 100, 1632: 33.33, 1633: 66.67, 1634: 100, 1635: 100, 1636: 100, 1637: 100, 1638: 0.0, 1639: 33.33, 1640: 100, 1641: 66.67, 1642: 100, 1643: 0.0, 1644: 100, 1645: 33.33, 1646: 0.0, 1647: 0.0, 1648: 66.67, 1649: 0.0, 1650: 0.0, 1651: 0.0, 1652: 0.0, 1653: 0.0, 1654: 100, 1655: 100, 1656: 33.33, 1657: 100, 1658: 100, 1659: 100, 1660: 66.67, 1661: 0.0, 1662: 33.33, 1663: 100, 1664: 0.0, 1665: 0.0, 1666: 33.33, 1667: 100, 1668: 33.33, 1669: 33.33, 1670: 100, 1671: 66.67, 1672: 0.0, 1673: 0.0, 1674: 33.33, 1675: 33.33, 1676: 100, 1677: 100, 1678: 0.0, 1679: 33.33, 1680: 33.33, 1681: 100, 1682: 66.67, 1683: 0.0, 1684: 100, 1685: 0.0, 1686: 0.0, 1687: 33.33, 1688: 0.0, 1689: 33.33, 1690: 33.33, 1691: 66.67, 1692: 0.0, 1693: 66.67, 1694: 33.33, 1695: 0.0, 1696: 33.33, 1697: 0.0, 1698: 100, 1699: 100, 1700: 100, 1701: 33.33, 1702: 100, 1703: 33.33, 1704: 0.0, 1705: 33.33, 1706: 100, 1707: 33.33, 1708: 0.0, 1709: 100, 1710: 33.33, 1711: 0.0, 1712: 100, 1713: 100, 1714: 100, 1715: 33.33, 1716: 0.0, 1717: 33.33, 1718: 100, 1719: 0.0, 1720: 100, 1721: 100, 1722: 0.0, 1723: 100, 1724: 100, 1725: 0.0, 1726: 0.0, 1727: 0.0, 1728: 0.0, 1729: 100, 1730: 33.33, 1731: 66.67, 1732: 100, 1733: 100, 1734: 100, 1735: 100, 1736: 0.0, 1737: 33.33, 1738: 100, 1739: 0.0, 1740: 33.33, 1741: 0.0, 1742: 33.33, 1743: 100, 1744: 100, 1745: 100, 1746: 0.0, 1747: 0.0, 1748: 66.67, 1749: 0.0, 1750: 100, 1751: 0.0, 1752: 100, 1753: 0.0, 1754: 100, 1755: 100, 1756: 100, 1757: 100, 1758: 0.0, 1759: 66.67, 1760: 33.33, 1761: 0.0, 1762: 33.33, 1763: 100, 1764: 33.33, 1765: 0.0, 1766: 0.0, 1767: 100, 1768: 0.0, 1769: 100, 1770: 66.67, 1771: 0.0, 1772: 0.0, 1773: 0.0, 1774: 100, 1775: 100, 1776: 100, 1777: 33.33, 1778: 0.0, 1779: 0.0, 1780: 100, 1781: 100, 1782: 0.0, 1783: 33.33, 1784: 0.0, 1785: 33.33, 1786: 0.0, 1787: 66.67, 1788: 100, 1789: 100, 1790: 100, 1791: 100, 1792: 0.0, 1793: 100, 1794: 100, 1795: 66.67, 1796: 66.67, 1797: 0.0, 1798: 100, 1799: 0.0, 1800: 33.33, 1801: 100, 1802: 100, 1803: 66.67, 1804: 100, 1805: 100, 1806: 100, 1807: 100, 1808: 0.0, 1809: 100, 1810: 66.67, 1811: 100, 1812: 0.0, 1813: 100, 1814: 100, 1815: 0.0, 1816: 100, 1817: 100, 1818: 66.67, 1819: 33.33, 1820: 100, 1821: 100, 1822: 100, 1823: 0.0, 1824: 0.0, 1825: 0.0, 1826: 0.0, 1827: 100, 1828: 0.0, 1829: 100, 1830: 100, 1831: 66.67, 1832: 100, 1833: 100, 1834: 0.0, 1835: 66.67, 1836: 100, 1837: 100, 1838: 0.0, 1839: 100, 1840: 100, 1841: 100, 1842: 0.0, 1843: 100, 1844: 100, 1845: 100, 1846: 100, 1847: 66.67, 1848: 100, 1849: 0.0, 1850: 100, 1851: 0.0, 1852: 100, 1853: 100, 1854: 100, 1855: 0.0, 1856: 100, 1857: 0.0, 1858: 100, 1859: 0.0, 1860: 100, 1861: 100, 1862: 100, 1863: 100, 1864: 100, 1865: 0.0, 1866: 100, 1867: 0.0, 1868: 66.67, 1869: 33.33, 1870: 33.33, 1871: 100, 1872: 100, 1873: 0.0, 1874: 100, 1875: 33.33, 1876: 0.0, 1877: 100, 1878: 100, 1879: 33.33, 1880: 100, 1881: 100, 1882: 0.0, 1883: 33.33, 1884: 0.0, 1885: 100, 1886: 0.0, 1887: 33.33, 1888: 100, 1889: 33.33, 1890: 100, 1891: 33.33, 1892: 0.0, 1893: 66.67, 1894: 0.0, 1895: 100, 1896: 100, 1897: 100, 1898: 33.33, 1899: 0.0, 1900: 100, 1901: 0.0, 1902: 0.0, 1903: 33.33, 1904: 100, 1905: 0.0, 1906: 100, 1907: 0.0, 1908: 0.0, 1909: 0.0, 1910: 0.0, 1911: 100, 1912: 33.33, 1913: 66.67, 1914: 100, 1915: 100, 1916: 100, 1917: 66.67, 1918: 66.67, 1919: 100, 1920: 100, 1921: 100, 1922: 100, 1923: 0.0, 1924: 100, 1925: 100, 1926: 0.0, 1927: 0.0, 1928: 100, 1929: 66.67, 1930: 100, 1931: 100, 1932: 33.33, 1933: 100, 1934: 66.67, 1935: 33.33, 1936: 0.0, 1937: 100, 1938: 100, 1939: 100, 1940: 0.0, 1941: 0.0, 1942: 100, 1943: 0.0, 1944: 100, 1945: 100, 1946: 33.33, 1947: 66.67, 1948: 0.0, 1949: 100, 1950: 33.33, 1951: 100, 1952: 0.0, 1953: 0.0, 1954: 66.67, 1955: 33.33, 1956: 0.0, 1957: 0.0, 1958: 100, 1959: 66.67, 1960: 33.33, 1961: 100, 1962: 100, 1963: 33.33, 1964: 0.0, 1965: 0.0, 1966: 0.0, 1967: 66.67, 1968: 33.33, 1969: 33.33, 1970: 66.67, 1971: 66.67, 1972: 66.67, 1973: 100, 1974: 100, 1975: 0.0, 1976: 0.0, 1977: 100, 1978: 100, 1979: 100, 1980: 0.0, 1981: 33.33, 1982: 100, 1983: 33.33, 1984: 100, 1985: 0.0, 1986: 0.0, 1987: 100, 1988: 0.0, 1989: 100, 1990: 0.0, 1991: 33.33, 1992: 100, 1993: 33.33, 1994: 100, 1995: 100, 1996: 100, 1997: 100, 1998: 100, 1999: 0.0, 2000: 0.0, 2001: 100, 2002: 0.0, 2003: 100, 2004: 100, 2005: 0.0, 2006: 100, 2007: 0.0, 2008: 66.67, 2009: 0.0, 2010: 100, 2011: 100, 2012: 100, 2013: 100, 2014: 0.0, 2015: 0.0, 2016: 33.33, 2017: 100, 2018: 33.33, 2019: 0.0, 2020: 100, 2021: 100, 2022: 100, 2023: 100, 2024: 100, 2025: 0.0, 2026: 0.0, 2027: 0.0, 2028: 0.0, 2029: 100, 2030: 100, 2031: 66.67, 2032: 100, 2033: 0.0, 2034: 100, 2035: 0.0, 2036: 100, 2037: 0.0, 2038: 100, 2039: 100, 2040: 0.0, 2041: 100, 2042: 66.67, 2043: 100, 2044: 100, 2045: 100, 2046: 0.0, 2047: 0.0, 2048: 100, 2049: 0.0, 2050: 100, 2051: 100, 2052: 100, 2053: 0.0, 2054: 100, 2055: 100, 2056: 100, 2057: 100, 2058: 100, 2059: 33.33, 2060: 100, 2061: 100, 2062: 33.33, 2063: 0.0, 2064: 100, 2065: 66.67, 2066: 33.33, 2067: 0.0, 2068: 66.67, 2069: 33.33, 2070: 100, 2071: 0.0, 2072: 0.0, 2073: 0.0, 2074: 33.33, 2075: 100, 2076: 66.67, 2077: 33.33, 2078: 100, 2079: 0.0, 2080: 100, 2081: 100, 2082: 100, 2083: 100, 2084: 33.33, 2085: 33.33, 2086: 33.33, 2087: 100, 2088: 66.67, 2089: 100, 2090: 0.0, 2091: 100, 2092: 0.0, 2093: 100, 2094: 33.33, 2095: 100, 2096: 100, 2097: 0.0, 2098: 100, 2099: 66.67, 2100: 0.0, 2101: 0.0, 2102: 100, 2103: 100, 2104: 0.0, 2105: 100, 2106: 100, 2107: 100, 2108: 100, 2109: 0.0, 2110: 100, 2111: 33.33, 2112: 0.0, 2113: 100, 2114: 66.67, 2115: 100, 2116: 0.0, 2117: 33.33, 2118: 100, 2119: 100, 2120: 100, 2121: 33.33, 2122: 100, 2123: 0.0, 2124: 0.0, 2125: 0.0, 2126: 100, 2127: 33.33, 2128: 100, 2129: 66.67, 2130: 0.0, 2131: 0.0, 2132: 0.0, 2133: 0.0, 2134: 0.0, 2135: 33.33, 2136: 100, 2137: 0.0, 2138: 0.0, 2139: 100, 2140: 0.0, 2141: 0.0, 2142: 100, 2143: 100, 2144: 0.0, 2145: 100, 2146: 0.0, 2147: 66.67, 2148: 100, 2149: 100, 2150: 0.0, 2151: 100, 2152: 0.0, 2153: 100, 2154: 0.0, 2155: 0.0, 2156: 100, 2157: 0.0, 2158: 100, 2159: 0.0, 2160: 33.33, 2161: 100, 2162: 100, 2163: 100, 2164: 33.33, 2165: 100, 2166: 33.33, 2167: 100, 2168: 100, 2169: 0.0, 2170: 100, 2171: 66.67, 2172: 0.0, 2173: 100, 2174: 0.0, 2175: 33.33, 2176: 100, 2177: 100, 2178: 0.0, 2179: 66.67, 2180: 100, 2181: 66.67, 2182: 0.0, 2183: 100, 2184: 66.67, 2185: 100, 2186: 66.67, 2187: 100, 2188: 100, 2189: 100, 2190: 66.67, 2191: 100, 2192: 33.33, 2193: 100, 2194: 0.0, 2195: 100, 2196: 100, 2197: 100, 2198: 100, 2199: 0.0, 2200: 66.67, 2201: 100, 2202: 100, 2203: 100, 2204: 0.0, 2205: 33.33, 2206: 0.0, 2207: 100, 2208: 100, 2209: 33.33, 2210: 100, 2211: 100, 2212: 100, 2213: 33.33, 2214: 0.0, 2215: 0.0, 2216: 100, 2217: 0.0, 2218: 0.0, 2219: 100, 2220: 0.0, 2221: 0.0, 2222: 100, 2223: 100, 2224: 66.67, 2225: 33.33, 2226: 100, 2227: 100, 2228: 100, 2229: 0.0, 2230: 0.0, 2231: 100, 2232: 100, 2233: 0.0, 2234: 100, 2235: 0.0, 2236: 100, 2237: 100, 2238: 33.33, 2239: 0.0, 2240: 66.67, 2241: 100, 2242: 100, 2243: 100, 2244: 0.0, 2245: 100, 2246: 100, 2247: 100, 2248: 100, 2249: 0.0, 2250: 0.0, 2251: 100, 2252: 33.33, 2253: 33.33, 2254: 100, 2255: 100, 2256: 66.67, 2257: 100, 2258: 100, 2259: 100, 2260: 100, 2261: 100, 2262: 100, 2263: 100, 2264: 0.0, 2265: 66.67, 2266: 100, 2267: 100, 2268: 0.0, 2269: 66.67, 2270: 0.0, 2271: 33.33, 2272: 100, 2273: 66.67, 2274: 0.0, 2275: 0.0, 2276: 100, 2277: 66.67, 2278: 0.0, 2279: 100, 2280: 0.0, 2281: 0.0, 2282: 0.0, 2283: 100, 2284: 33.33, 2285: 100, 2286: 100, 2287: 100, 2288: 0.0, 2289: 0.0, 2290: 100, 2291: 100, 2292: 100, 2293: 66.67, 2294: 100, 2295: 0.0, 2296: 0.0, 2297: 100, 2298: 100, 2299: 0.0, 2300: 100, 2301: 0.0, 2302: 100, 2303: 100, 2304: 33.33, 2305: 100, 2306: 0.0, 2307: 0.0, 2308: 100, 2309: 33.33, 2310: 100, 2311: 33.33, 2312: 33.33, 2313: 100, 2314: 100, 2315: 100, 2316: 0.0, 2317: 0.0, 2318: 100, 2319: 0.0, 2320: 100, 2321: 66.67, 2322: 33.33, 2323: 100, 2324: 0.0, 2325: 0.0, 2326: 100, 2327: 33.33, 2328: 0.0, 2329: 100, 2330: 100, 2331: 100, 2332: 0.0, 2333: 33.33, 2334: 100, 2335: 100, 2336: 0.0, 2337: 100, 2338: 100, 2339: 33.33, 2340: 100, 2341: 100, 2342: 100, 2343: 100, 2344: 66.67, 2345: 33.33, 2346: 66.67, 2347: 33.33, 2348: 33.33, 2349: 0.0, 2350: 100, 2351: 66.67, 2352: 0.0, 2353: 0.0, 2354: 0.0, 2355: 100, 2356: 100, 2357: 33.33, 2358: 0.0, 2359: 100, 2360: 100, 2361: 100, 2362: 66.67, 2363: 100, 2364: 0.0, 2365: 0.0, 2366: 0.0, 2367: 100, 2368: 100, 2369: 33.33, 2370: 66.67, 2371: 100, 2372: 100, 2373: 0.0, 2374: 100, 2375: 100, 2376: 0.0, 2377: 100, 2378: 100, 2379: 0.0, 2380: 66.67, 2381: 0.0, 2382: 0.0, 2383: 100, 2384: 100, 2385: 100, 2386: 100, 2387: 100, 2388: 100, 2389: 0.0, 2390: 0.0, 2391: 66.67, 2392: 0.0, 2393: 0.0, 2394: 0.0, 2395: 66.67, 2396: 66.67, 2397: 100, 2398: 100, 2399: 0.0, 2400: 100, 2401: 33.33, 2402: 100, 2403: 0.0, 2404: 0.0, 2405: 0.0, 2406: 100, 2407: 100, 2408: 100, 2409: 100, 2410: 100, 2411: 0.0, 2412: 100, 2413: 0.0, 2414: 100, 2415: 100, 2416: 100, 2417: 33.33, 2418: 0.0, 2419: 100, 2420: 100, 2421: 100, 2422: 33.33, 2423: 100, 2424: 0.0, 2425: 33.33, 2426: 100, 2427: 0.0, 2428: 100, 2429: 100, 2430: 0.0, 2431: 100, 2432: 100, 2433: 100, 2434: 66.67, 2435: 100, 2436: 100, 2437: 0.0, 2438: 100, 2439: 0.0, 2440: 0.0, 2441: 100, 2442: 33.33, 2443: 33.33, 2444: 0.0, 2445: 100, 2446: 100, 2447: 0.0, 2448: 0.0, 2449: 100, 2450: 66.67, 2451: 0.0, 2452: 100, 2453: 33.33, 2454: 100, 2455: 100, 2456: 33.33, 2457: 0.0, 2458: 100, 2459: 100, 2460: 100, 2461: 0.0, 2462: 100, 2463: 0.0, 2464: 100, 2465: 100, 2466: 0.0, 2467: 33.33, 2468: 33.33, 2469: 33.33, 2470: 33.33, 2471: 0.0, 2472: 100, 2473: 100, 2474: 0.0, 2475: 33.33, 2476: 100, 2477: 33.33, 2478: 100, 2479: 0.0, 2480: 33.33, 2481: 100, 2482: 0.0, 2483: 0.0, 2484: 33.33, 2485: 100, 2486: 100, 2487: 0.0, 2488: 33.33, 2489: 0.0, 2490: 100, 2491: 100, 2492: 100, 2493: 0.0, 2494: 0.0, 2495: 100, 2496: 100, 2497: 100, 2498: 100, 2499: 0.0, 2500: 33.33, 2501: 33.33, 2502: 33.33, 2503: 100, 2504: 66.67, 2505: 100, 2506: 100, 2507: 100, 2508: 33.33, 2509: 0.0, 2510: 0.0, 2511: 100, 2512: 0.0, 2513: 100, 2514: 100, 2515: 33.33, 2516: 100, 2517: 100, 2518: 0.0, 2519: 100, 2520: 100, 2521: 33.33, 2522: 100, 2523: 100, 2524: 0.0, 2525: 0.0, 2526: 0.0, 2527: 100, 2528: 66.67, 2529: 33.33, 2530: 100, 2531: 33.33, 2532: 0.0, 2533: 100, 2534: 100, 2535: 100, 2536: 0.0, 2537: 0.0, 2538: 100, 2539: 100, 2540: 0.0, 2541: 100, 2542: 66.67, 2543: 0.0, 2544: 100, 2545: 66.67, 2546: 100, 2547: 100, 2548: 66.67, 2549: 66.67, 2550: 66.67, 2551: 100, 2552: 0.0, 2553: 66.67, 2554: 0.0, 2555: 0.0, 2556: 33.33, 2557: 100, 2558: 0.0, 2559: 0.0, 2560: 0.0, 2561: 100, 2562: 100, 2563: 100, 2564: 0.0, 2565: 100, 2566: 0.0, 2567: 100, 2568: 33.33, 2569: 100, 2570: 0.0, 2571: 66.67, 2572: 0.0, 2573: 100, 2574: 100, 2575: 33.33, 2576: 100, 2577: 33.33, 2578: 0.0, 2579: 66.67, 2580: 100, 2581: 100, 2582: 66.67, 2583: 100, 2584: 100, 2585: 0.0, 2586: 100, 2587: 0.0, 2588: 100, 2589: 66.67, 2590: 0.0, 2591: 66.67, 2592: 66.67, 2593: 33.33, 2594: 100, 2595: 100, 2596: 100, 2597: 33.33, 2598: 100, 2599: 66.67, 2600: 66.67, 2601: 0.0, 2602: 100, 2603: 100, 2604: 100, 2605: 100, 2606: 0.0, 2607: 66.67, 2608: 100, 2609: 66.67, 2610: 0.0, 2611: 0.0, 2612: 0.0, 2613: 100, 2614: 100, 2615: 0.0, 2616: 100, 2617: 33.33, 2618: 100, 2619: 0.0, 2620: 0.0, 2621: 100, 2622: 0.0, 2623: 100, 2624: 100, 2625: 100, 2626: 33.33, 2627: 0.0, 2628: 100, 2629: 0.0, 2630: 100, 2631: 100, 2632: 0.0, 2633: 0.0, 2634: 100, 2635: 33.33, 2636: 100, 2637: 100, 2638: 0.0, 2639: 66.67, 2640: 0.0, 2641: 0.0, 2642: 0.0, 2643: 0.0, 2644: 100, 2645: 33.33, 2646: 33.33, 2647: 33.33, 2648: 100, 2649: 0.0, 2650: 0.0, 2651: 0.0, 2652: 100, 2653: 100, 2654: 100, 2655: 100, 2656: 100, 2657: 100, 2658: 100, 2659: 33.33, 2660: 0.0, 2661: 0.0, 2662: 66.67, 2663: 100, 2664: 100, 2665: 0.0, 2666: 33.33, 2667: 100, 2668: 100, 2669: 100, 2670: 0.0, 2671: 66.67, 2672: 100, 2673: 33.33, 2674: 33.33, 2675: 66.67, 2676: 0.0, 2677: 66.67, 2678: 0.0, 2679: 100, 2680: 0.0, 2681: 66.67, 2682: 0.0, 2683: 66.67, 2684: 100, 2685: 100, 2686: 100, 2687: 100, 2688: 100, 2689: 100, 2690: 100, 2691: 33.33, 2692: 100, 2693: 0.0, 2694: 33.33, 2695: 100, 2696: 66.67, 2697: 0.0, 2698: 100, 2699: 33.33, 2700: 100, 2701: 0.0, 2702: 100, 2703: 100, 2704: 66.67, 2705: 0.0, 2706: 100, 2707: 100, 2708: 0.0, 2709: 0.0, 2710: 100, 2711: 0.0, 2712: 100, 2713: 100, 2714: 0.0, 2715: 100, 2716: 0.0, 2717: 100, 2718: 100, 2719: 100, 2720: 66.67, 2721: 100, 2722: 0.0, 2723: 100, 2724: 100, 2725: 0.0, 2726: 33.33, 2727: 0.0, 2728: 100, 2729: 0.0, 2730: 66.67, 2731: 100, 2732: 100, 2733: 0.0, 2734: 100, 2735: 100, 2736: 100, 2737: 0.0, 2738: 100, 2739: 0.0, 2740: 0.0, 2741: 100, 2742: 66.67, 2743: 100, 2744: 100, 2745: 0.0, 2746: 100, 2747: 0.0, 2748: 0.0, 2749: 66.67, 2750: 100, 2751: 100, 2752: 100, 2753: 0.0, 2754: 100, 2755: 66.67, 2756: 33.33, 2757: 33.33, 2758: 66.67, 2759: 100, 2760: 66.67, 2761: 100, 2762: 66.67, 2763: 0.0, 2764: 100, 2765: 0.0, 2766: 0.0, 2767: 100, 2768: 100, 2769: 0.0, 2770: 0.0, 2771: 100, 2772: 100, 2773: 100, 2774: 100, 2775: 0.0, 2776: 100, 2777: 0.0, 2778: 0.0, 2779: 100, 2780: 100, 2781: 100, 2782: 100, 2783: 0.0, 2784: 100, 2785: 33.33, 2786: 100, 2787: 0.0, 2788: 66.67, 2789: 100, 2790: 100, 2791: 100, 2792: 100, 2793: 33.33, 2794: 0.0, 2795: 100, 2796: 100, 2797: 0.0, 2798: 100, 2799: 100, 2800: 66.67, 2801: 100, 2802: 100, 2803: 100, 2804: 33.33, 2805: 100, 2806: 100, 2807: 66.67, 2808: 100, 2809: 0.0, 2810: 0.0, 2811: 100, 2812: 0.0, 2813: 0.0, 2814: 100, 2815: 100, 2816: 0.0, 2817: 0.0, 2818: 0.0, 2819: 0.0, 2820: 0.0, 2821: 100, 2822: 100, 2823: 33.33, 2824: 66.67, 2825: 100, 2826: 0.0, 2827: 100, 2828: 100, 2829: 100, 2830: 100, 2831: 0.0, 2832: 100, 2833: 100, 2834: 0.0, 2835: 100, 2836: 33.33, 2837: 100, 2838: 100, 2839: 33.33, 2840: 100, 2841: 0.0, 2842: 0.0, 2843: 100, 2844: 0.0, 2845: 100, 2846: 0.0, 2847: 100, 2848: 100, 2849: 0.0, 2850: 100, 2851: 0.0, 2852: 100, 2853: 100, 2854: 0.0, 2855: 100, 2856: 33.33, 2857: 0.0, 2858: 100, 2859: 33.33, 2860: 0.0, 2861: 33.33, 2862: 100, 2863: 66.67, 2864: 33.33, 2865: 33.33, 2866: 0.0, 2867: 100, 2868: 0.0, 2869: 100, 2870: 0.0, 2871: 66.67, 2872: 100, 2873: 0.0, 2874: 0.0, 2875: 33.33, 2876: 100, 2877: 0.0, 2878: 100, 2879: 0.0, 2880: 100, 2881: 0.0, 2882: 33.33, 2883: 0.0, 2884: 100, 2885: 0.0, 2886: 100, 2887: 33.33, 2888: 100, 2889: 100, 2890: 100, 2891: 0.0, 2892: 0.0, 2893: 100, 2894: 66.67, 2895: 0.0, 2896: 100, 2897: 66.67, 2898: 100, 2899: 100, 2900: 100, 2901: 66.67, 2902: 0.0, 2903: 0.0, 2904: 66.67, 2905: 0.0, 2906: 66.67, 2907: 100, 2908: 66.67, 2909: 100, 2910: 100, 2911: 100, 2912: 33.33, 2913: 33.33, 2914: 100, 2915: 100, 2916: 100, 2917: 100, 2918: 0.0, 2919: 33.33, 2920: 0.0, 2921: 100, 2922: 0.0, 2923: 100, 2924: 100, 2925: 100, 2926: 0.0, 2927: 100, 2928: 33.33, 2929: 33.33, 2930: 100, 2931: 100, 2932: 100, 2933: 33.33, 2934: 100, 2935: 66.67, 2936: 100, 2937: 0.0, 2938: 33.33, 2939: 100, 2940: 33.33, 2941: 100, 2942: 0.0, 2943: 0.0, 2944: 0.0, 2945: 0.0, 2946: 0.0, 2947: 0.0, 2948: 100, 2949: 100, 2950: 100, 2951: 100, 2952: 100, 2953: 100, 2954: 0.0, 2955: 100, 2956: 100, 2957: 100, 2958: 66.67, 2959: 66.67, 2960: 0.0, 2961: 0.0, 2962: 100, 2963: 0.0, 2964: 66.67, 2965: 0.0, 2966: 0.0, 2967: 100, 2968: 100, 2969: 100, 2970: 33.33, 2971: 100, 2972: 33.33, 2973: 0.0, 2974: 100, 2975: 0.0, 2976: 66.67, 2977: 66.67, 2978: 33.33, 2979: 0.0, 2980: 0.0, 2981: 0.0, 2982: 100, 2983: 0.0, 2984: 100, 2985: 66.67, 2986: 0.0, 2987: 100, 2988: 0.0, 2989: 0.0, 2990: 100, 2991: 33.33, 2992: 100, 2993: 100, 2994: 0.0, 2995: 100, 2996: 100, 2997: 0.0, 2998: 0.0, 2999: 66.67, 3000: 0.0, 3001: 100, 3002: 0.0, 3003: 0.0, 3004: 66.67, 3005: 33.33, 3006: 66.67, 3007: 100, 3008: 100, 3009: 0.0, 3010: 33.33, 3011: 100, 3012: 100, 3013: 100, 3014: 100, 3015: 100, 3016: 100, 3017: 100, 3018: 33.33, 3019: 33.33, 3020: 100, 3021: 100, 3022: 33.33, 3023: 100, 3024: 33.33, 3025: 100, 3026: 33.33, 3027: 100, 3028: 0.0, 3029: 100, 3030: 100, 3031: 100, 3032: 100, 3033: 0.0, 3034: 0.0, 3035: 100, 3036: 100, 3037: 0.0, 3038: 100, 3039: 0.0, 3040: 66.67, 3041: 100, 3042: 100, 3043: 0.0, 3044: 66.67, 3045: 0.0, 3046: 0.0, 3047: 100, 3048: 0.0, 3049: 100, 3050: 0.0, 3051: 0.0, 3052: 100, 3053: 66.67, 3054: 33.33, 3055: 100, 3056: 0.0, 3057: 100, 3058: 0.0, 3059: 0.0, 3060: 100, 3061: 100, 3062: 66.67, 3063: 100, 3064: 0.0, 3065: 0.0, 3066: 66.67, 3067: 66.67, 3068: 0.0, 3069: 100, 3070: 33.33, 3071: 100, 3072: 0.0, 3073: 100, 3074: 100, 3075: 33.33, 3076: 100, 3077: 0.0, 3078: 0.0, 3079: 33.33, 3080: 0.0, 3081: 0.0, 3082: 100, 3083: 0.0, 3084: 100, 3085: 100, 3086: 0.0, 3087: 100, 3088: 100, 3089: 0.0, 3090: 66.67, 3091: 100, 3092: 33.33, 3093: 0.0, 3094: 100, 3095: 100, 3096: 0.0, 3097: 0.0, 3098: 0.0, 3099: 66.67, 3100: 0.0, 3101: 0.0, 3102: 33.33, 3103: 0.0, 3104: 100, 3105: 0.0, 3106: 0.0, 3107: 33.33, 3108: 100, 3109: 66.67, 3110: 0.0, 3111: 0.0, 3112: 100, 3113: 100, 3114: 100, 3115: 100, 3116: 100, 3117: 0.0, 3118: 100, 3119: 100, 3120: 0.0, 3121: 33.33, 3122: 100, 3123: 0.0, 3124: 33.33, 3125: 33.33, 3126: 0.0, 3127: 100, 3128: 33.33}\n",
      "55.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "judge=VQAEval(gt_label,rsps)\n",
    "judge.evaluate()\n",
    "print(judge.evalQA)\n",
    "print(judge.accuracy['overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_scores=[judge.evalQA[i] for i in range(len(judge.evalQA))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct=[]\n",
    "threshold=30\n",
    "for i,x in enumerate(gt_scores):\n",
    "    if x >=threshold:\n",
    "        is_correct.append(1)\n",
    "    else:\n",
    "        is_correct.append(0)\n",
    "is_correct=np.array(is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3129,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3129 responses. 2084 answer correctly and 1045 answer wrongly\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(is_correct)} responses. {sum(is_correct==1)} answer correctly and {sum(is_correct==0)} answer wrongly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset length:503\n",
      "validset length:2000\n",
      "testset length:626\n",
      "trainset postive: 334 negative: 169\n",
      "valset postive: 1317 negative: 683\n",
      "testset postive: 433 negative: 193\n"
     ]
    }
   ],
   "source": [
    "length=len(is_correct)\n",
    "train_rate=0.8\n",
    "\n",
    "all_indices = np.random.permutation(length)\n",
    "train_val_idxs = all_indices[:int(\n",
    "    train_rate * length)]  # trainset and validation index\n",
    "\n",
    "test_idxs=all_indices[int(\n",
    "    train_rate * length):]  #test index\n",
    "\n",
    "validset_len = 2000\n",
    "# exclude validation samples.\n",
    "train_idxs = train_val_idxs[:- validset_len]  # trainset index\n",
    "val_idxs = train_val_idxs[- validset_len:]  # validation index\n",
    "hall_label_test = []\n",
    "hall_label_wild = []\n",
    "hall_label_val = []\n",
    "\n",
    "'''get testset, wildset and valset. The valset is used for determining the hype-parameters'''\n",
    "hall_label_test = is_correct[test_idxs]\n",
    "hall_label_wild = is_correct[train_idxs]\n",
    "hall_label_val = is_correct[val_idxs]\n",
    "\n",
    "print(f'trainset length:{len(hall_label_wild)}')\n",
    "print(f'validset length:{len(hall_label_val)}')\n",
    "print(f'testset length:{len(hall_label_test)}')\n",
    "print(f'trainset postive: {sum(hall_label_wild==1)} negative: {sum(hall_label_wild==0)}')\n",
    "print(f'valset postive: {sum(hall_label_val==1)} negative: {sum(hall_label_val==0)}')\n",
    "print(\n",
    "    f'testset postive: {sum(hall_label_test==1)} negative: {sum(hall_label_test==0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3129, 29, 3584)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings = embeddings\n",
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train = all_embeddings[train_idxs]\n",
    "embedding_val = all_embeddings[val_idxs]\n",
    "embedding_test = all_embeddings[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ml_utils.grid_search import GridSearch\n",
    "from ml_utils.PCA_discriminator import PCALinear,PCAKernel,KernelPCA\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,roc_curve,f1_score,balanced_accuracy_score\n",
    "from ml_utils.metrics import auc_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search for Best 'n_components' & 'layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 29, 3584)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "M=3584\n",
    "def evaluator(n_components,i_layer,X,y):\n",
    "    discriminator=KernelPCA(X[:,i_layer,:],n_components,M,method='CoP',gamma=1)\n",
    "    scores=discriminator.get_score(X[:,i_layer,:])\n",
    "    \n",
    "    return roc_auc_score(y,scores)\n",
    "    #split=discriminator.get_best_split(scores,y)\n",
    "    #y_pred=scores>split\n",
    "    #return accuracy_score(y,y_pred)\n",
    "    #return balanced_accuracy_score(y,y_pred)\n",
    "    #\n",
    "    #split=discriminator.get_best_split(y)\n",
    "    #y_preds=(scores>split)\n",
    "    #return accuracy_score(y,y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Searching for best n_components,layer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcc67378e2b41449d4f3dc5e7df9ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0530192867234280bdf387c7e03f620d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found best n_components:1,layer:10 Score: 0.5212454322404061\n",
      "found best n_components:1,layer:11 Score: 0.5234377345024129\n",
      "found best n_components:1,layer:12 Score: 0.5277734235601343\n",
      "found best n_components:1,layer:13 Score: 0.5346360411379071\n",
      "found best n_components:1,layer:15 Score: 0.5357733257292017\n",
      "found best n_components:1,layer:17 Score: 0.5440155817994444\n",
      "found best n_components:1,layer:20 Score: 0.5463679710420439\n",
      "found best n_components:1,layer:21 Score: 0.5478676747699583\n",
      "found best n_components:1,layer:27 Score: 0.5577663864032791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41b7f1572394b90b8b01f0575df9ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found best n_components:2,layer:17 Score: 0.5623644402347499\n",
      "found best n_components:2,layer:20 Score: 0.5624778351793364\n",
      "found best n_components:2,layer:22 Score: 0.5625634372453477\n",
      "found best n_components:2,layer:24 Score: 0.5665400423118784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9979d1d52c34074975933a1db05f915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found best n_components:3,layer:12 Score: 0.5677051197817481\n",
      "found best n_components:3,layer:13 Score: 0.5717072942965679\n",
      "found best n_components:3,layer:17 Score: 0.5727484155279925\n"
     ]
    }
   ],
   "source": [
    "# graid search for best hyper-parameters on validation set\n",
    "grid={\n",
    "    'n_components':range(1,4),\n",
    "    'layer':range(10,embedding_val.shape[1])\n",
    "}\n",
    "grid_search=GridSearch(evaluator,grid,embedding_val,hall_label_val)\n",
    "best_paras=grid_search.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_components': 3, 'layer': 17, 'best_score': np.float64(0.5727484155279925)}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_layer=best_paras['layer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Results on Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator=KernelPCA(embedding_val[:,best_layer,:],best_paras['n_components'],M,method='CoP',gamma=1)\n",
    "scores=discriminator.get_score(embedding_val[:,best_layer,:])\n",
    "best_split=discriminator.get_best_split(scores,hall_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set acc: 0.3359840954274354 AUROC:0.5336073415299578\n"
     ]
    }
   ],
   "source": [
    "discriminator=KernelPCA(embedding_train[:,best_layer,:],best_paras['n_components'],M,method='CoP',gamma=1)\n",
    "scores=discriminator.get_score(embedding_train[:,best_layer,:])\n",
    "y_train=(scores>best_split)\n",
    "print(f'train set acc: {accuracy_score(hall_label_wild,y_train)} AUROC:{roc_auc_score(hall_label_wild,scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set acc: 0.3083067092651757 AUROC:0.6140195527049503\n"
     ]
    }
   ],
   "source": [
    "discriminator=KernelPCA(embedding_test[:,best_layer,:],best_paras['n_components'],M,method='CoP',gamma=1)\n",
    "scores=discriminator.get_score(embedding_test[:,best_layer,:])\n",
    "preds=(scores>best_split)\n",
    "print(f'test set acc: {accuracy_score(hall_label_test,preds)} AUROC:{roc_auc_score(hall_label_test,scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Linear Prob to Get Better Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=embedding_train[:, best_layer,:]\n",
    "y_train=y_train\n",
    "\n",
    "X_test=embedding_test[:, best_layer,:]\n",
    "y_test=hall_label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc, final_acc, (\n",
    "            clf, best_state, best_preds, preds, labels_val), losses_train = get_linear_acc(\n",
    "            X_train,\n",
    "            y_train,\n",
    "           X_train,\n",
    "            y_train,\n",
    "            2, epochs=50,\n",
    "            print_ret=True,\n",
    "            batch_size=512,\n",
    "            cosine=True,\n",
    "            nonlinear=True,\n",
    "            learning_rate=0.05,\n",
    "            weight_decay=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set acc: 0.3083067092651757 f1:0.0 auroc:0.4410606803958406  b-acc:0.5 auc-pr:0.6131639213444567\n"
     ]
    }
   ],
   "source": [
    "clf.eval()\n",
    "output = clf(torch.from_numpy(X_test).to(torch.float32).cuda())\n",
    "scores_test = torch.sigmoid(output).cpu().data.numpy()\n",
    "pred_test=scores_test>0.5\n",
    "print(f'test set acc: {accuracy_score(y_test,pred_test)} f1:{f1_score(y_test,pred_test)} auroc:{roc_auc_score(y_test,scores_test)}  b-acc:{balanced_accuracy_score(y_test,pred_test)} auc-pr:{auc_pr(y_test,scores_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

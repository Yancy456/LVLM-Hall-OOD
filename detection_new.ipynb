{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from utils.store_data import ReadData\n",
    "import numpy as np\n",
    "from llm.prompt_loader import PromptLoader\n",
    "import torch\n",
    "from utils.arguments import Arguments\n",
    "from tqdm import tqdm\n",
    "from llm.ml_tools import svd_embed_score\n",
    "from sklearn.decomposition import PCA\n",
    "from metric_utils import get_measures, print_measures\n",
    "from linear_probe import get_linear_acc\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img_path': '/root/autodl-fs/coco_images/val/COCO_val2014_000000310196.jpg',\n",
       " 'question': 'Is there a snowboard in the image?\\nAnswer the question using a single word or phrase.',\n",
       " 'label': 1,\n",
       " 'question_id': 1,\n",
       " 'category': 'popular',\n",
       " 'most_likely': {'embedding': array([[[ 3.8909912e-03,  1.4953613e-03,  5.4626465e-03, ...,\n",
       "           -4.2114258e-03,  1.5075684e-02,  2.4261475e-03]],\n",
       "  \n",
       "         [[-5.3787231e-03, -1.2474060e-03,  8.8424683e-03, ...,\n",
       "           -1.1787415e-02,  8.6975098e-03,  9.3383789e-03]],\n",
       "  \n",
       "         [[-3.2531738e-02,  3.3264160e-03,  1.9363403e-02, ...,\n",
       "            8.7356567e-03,  2.6062012e-02,  9.3231201e-03]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[ 1.5664062e+00,  2.2695312e+00, -4.0673828e-01, ...,\n",
       "           -2.7734375e+00, -1.3789062e+00, -2.1718750e+00]],\n",
       "  \n",
       "         [[ 2.2363281e+00,  2.5859375e+00, -5.2978516e-01, ...,\n",
       "           -2.6347656e+00, -1.5664062e+00, -2.3593750e+00]],\n",
       "  \n",
       "         [[ 8.9501953e-01,  3.9526367e-01,  9.9853516e-01, ...,\n",
       "           -5.2783203e-01,  3.5180664e-01, -1.5263672e+00]]], dtype=float32),\n",
       "  'response': 'yes'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reader=ReadData('/home/hallscope/output/pope')\n",
    "data=data_reader.read_all()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>question_id</th>\n",
       "      <th>category</th>\n",
       "      <th>most_likely</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/root/autodl-fs/coco_images/val/COCO_val2014_0...</td>\n",
       "      <td>Is there a snowboard in the image?\\nAnswer the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>popular</td>\n",
       "      <td>{'embedding': [[[ 0.00389099  0.00149536  0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/root/autodl-fs/coco_images/val/COCO_val2014_0...</td>\n",
       "      <td>Is there a dining table in the image?\\nAnswer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>popular</td>\n",
       "      <td>{'embedding': [[[ 0.00389099  0.00149536  0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/root/autodl-fs/coco_images/val/COCO_val2014_0...</td>\n",
       "      <td>Is there a person in the image?\\nAnswer the qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>popular</td>\n",
       "      <td>{'embedding': [[[ 0.00389099  0.00149536  0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/root/autodl-fs/coco_images/val/COCO_val2014_0...</td>\n",
       "      <td>Is there a car in the image?\\nAnswer the quest...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>popular</td>\n",
       "      <td>{'embedding': [[[ 0.00389099  0.00149536  0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/root/autodl-fs/coco_images/val/COCO_val2014_0...</td>\n",
       "      <td>Is there a skis in the image?\\nAnswer the ques...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>popular</td>\n",
       "      <td>{'embedding': [[[ 0.00389099  0.00149536  0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  \\\n",
       "0  /root/autodl-fs/coco_images/val/COCO_val2014_0...   \n",
       "1  /root/autodl-fs/coco_images/val/COCO_val2014_0...   \n",
       "2  /root/autodl-fs/coco_images/val/COCO_val2014_0...   \n",
       "3  /root/autodl-fs/coco_images/val/COCO_val2014_0...   \n",
       "4  /root/autodl-fs/coco_images/val/COCO_val2014_0...   \n",
       "\n",
       "                                            question  label  question_id  \\\n",
       "0  Is there a snowboard in the image?\\nAnswer the...      1            1   \n",
       "1  Is there a dining table in the image?\\nAnswer ...      0            2   \n",
       "2  Is there a person in the image?\\nAnswer the qu...      1            3   \n",
       "3  Is there a car in the image?\\nAnswer the quest...      0            4   \n",
       "4  Is there a skis in the image?\\nAnswer the ques...      1            5   \n",
       "\n",
       "  category                                        most_likely  \n",
       "0  popular  {'embedding': [[[ 0.00389099  0.00149536  0.00...  \n",
       "1  popular  {'embedding': [[[ 0.00389099  0.00149536  0.00...  \n",
       "2  popular  {'embedding': [[[ 0.00389099  0.00149536  0.00...  \n",
       "3  popular  {'embedding': [[[ 0.00389099  0.00149536  0.00...  \n",
       "4  popular  {'embedding': [[[ 0.00389099  0.00149536  0.00...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label=df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset length:1200\n",
      "validset length:1500\n",
      "testset length:300\n",
      "trainset postive: 589 negative: 611\n",
      "testset postive: 145 negative: 155\n"
     ]
    }
   ],
   "source": [
    "length=len(data)\n",
    "wild_ratio=0.9\n",
    "\n",
    "all_indices = np.random.permutation(length)\n",
    "train_val_idxs = all_indices[:int(\n",
    "    wild_ratio * length)]  # trainset and validation index\n",
    "\n",
    "test_idxs=all_indices[int(\n",
    "    wild_ratio * length):]  #test index\n",
    "\n",
    "validset_len = 1500\n",
    "# exclude validation samples.\n",
    "train_idxs = train_val_idxs[:len(\n",
    "    train_val_idxs) - validset_len]  # trainset index\n",
    "val_idxs = train_val_idxs[len(\n",
    "    train_val_idxs) - validset_len:]  # validation index\n",
    "gt_label_test = []\n",
    "gt_label_wild = []\n",
    "gt_label_val = []\n",
    "\n",
    "for i in range(length):\n",
    "    if i not in train_val_idxs:\n",
    "        gt_label_test.extend(gt_label[i: i+1])\n",
    "    elif i in train_idxs:\n",
    "        gt_label_wild.extend(gt_label[i: i+1])\n",
    "    else:\n",
    "        gt_label_val.extend(gt_label[i: i+1])\n",
    "\n",
    "'''get testset, wildset and valset. The valset is used for determining the hype-parameters'''\n",
    "gt_label_test = gt_label[test_idxs]\n",
    "gt_label_wild = gt_label[train_idxs]\n",
    "gt_label_val = gt_label[val_idxs]\n",
    "\n",
    "print(f'trainset length:{len(gt_label_wild)}')\n",
    "print(f'validset length:{len(gt_label_val)}')\n",
    "print(f'testset length:{len(gt_label_test)}')\n",
    "print(f'trainset postive: {sum(gt_label_wild==1)} negative: {sum(gt_label_wild==0)}')\n",
    "print(\n",
    "    f'testset postive: {sum(gt_label_test==1)} negative: {sum(gt_label_test==0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 33, 4096)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_generated = df['most_likely'].apply(lambda x: np.squeeze(x['embedding'])).to_list()\n",
    "embed_generated =np.stack(embed_generated)\n",
    "embed_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_generated_wild = embed_generated[train_idxs][:, 1:, :]\n",
    "embed_generated_eval = embed_generated[val_idxs][:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "from ml_utils.grid_search import GridSearch\n",
    "from ml_utils.PCA_discriminator import PCADiscriminator\n",
    "from ml_utils.metrics import auroc,get_best_split_from_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search for Best 'n_components' & 'layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(n_components,i_layer,X,y):\n",
    "    discriminator=PCADiscriminator(n_components,X[:,i_layer,:])\n",
    "    scores=discriminator.get_score()\n",
    "    return auroc(scores,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Searching for best n_components,layer\n",
      "found best n_components:1,layer:0 Score: 0.49121555765824093\n",
      "found best n_components:1,layer:1 Score: 0.4963770178072153\n",
      "found best n_components:1,layer:2 Score: 0.5189490683760076\n",
      "found best n_components:1,layer:3 Score: 0.519904169719908\n",
      "found best n_components:1,layer:5 Score: 0.5217076571737538\n",
      "found best n_components:1,layer:17 Score: 0.8614782905642391\n",
      "found best n_components:1,layer:20 Score: 0.8810338571865596\n",
      "found best n_components:1,layer:21 Score: 0.9026490278242185\n",
      "found best n_components:1,layer:22 Score: 0.9275439133187726\n",
      "found best n_components:1,layer:26 Score: 0.9345764472364312\n",
      "found best n_components:1,layer:27 Score: 0.9445721074835836\n",
      "found best n_components:1,layer:30 Score: 0.9472364311580026\n"
     ]
    }
   ],
   "source": [
    "# graid search for best hyper-parameters on validation set\n",
    "grid={\n",
    "    'n_components':range(1,12),\n",
    "    'layer':range(embed_generated_eval.shape[1])\n",
    "}\n",
    "grid_search=GridSearch(evaluator,grid,embed_generated_eval,gt_label_val)\n",
    "best_paras=grid_search.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_components': 1, 'layer': 30, 'best_score': np.float64(0.9472364311580026)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_layer=best_paras['layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator=PCADiscriminator(best_paras['n_components'],embed_generated_eval[:,best_layer,:])\n",
    "best_split=discriminator.get_best_split(gt_label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(32236.715)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator=PCADiscriminator(best_paras['n_components'],embed_generated_wild[:,best_layer,:])\n",
    "scores=discriminator.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False, False,  True])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=(scores>best_split)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8558333333333333"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(gt_label_wild,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9371149747554038)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(gt_label_wild,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=best_parameters['k'], whiten=False).fit(\n",
    "    embed_generated_wild[:, best_parameters['best_layer'], :])\n",
    "projection = pca_model.components_.T\n",
    "if weighted_svd:\n",
    "    projection = pca_model.singular_values_ * projection\n",
    "scores = np.mean(np.matmul(\n",
    "    embed_generated_wild[:, best_parameters['best_layer'], :], projection), -1, keepdims=True)\n",
    "assert scores.shape[1] == 1\n",
    "best_scores = np.sqrt(np.sum(np.square(scores), axis=1)\n",
    "                      ) * best_parameters['best_sign']\n",
    "\n",
    "'''get score for direct projection'''\n",
    "# direct projection\n",
    "\n",
    "embed_generated_test = embed_generated[test_idxs][:, 1:, :]\n",
    "\n",
    "test_scores = np.mean(np.matmul(embed_generated_test[:, best_parameters['best_layer'], :],\n",
    "                                projection), -1, keepdims=True)\n",
    "\n",
    "assert test_scores.shape[1] == 1\n",
    "test_scores = np.sqrt(np.sum(np.square(test_scores), axis=1))\n",
    "\n",
    "measures = get_measures(best_parameters['best_sign'] * test_scores[gt_label_test == 1],\n",
    "                        best_parameters['best_sign'] * test_scores[gt_label_test == 0], plot=False)\n",
    "print_measures(measures[0], measures[1], measures[2], 'direct-projection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score distributes in range: 8639.2412109375-1.1222127676010132\n"
     ]
    }
   ],
   "source": [
    "print(f'best_score distributes in range: {best_scores.max()}-{best_scores.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_indices_test = []\n",
    "for i in range(length):\n",
    "    if i not in train_val_idxs:\n",
    "        feat_indices_test.extend(np.arange(1 * i, 1 * i + 1).tolist())\n",
    "\n",
    "embed_generated_test = embed_generated[feat_indices_test][:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n",
      "postive: 2338 negative: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38 [00:48<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# gt training, saplma\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# embed_train = embed_generated_wild[:,layer,:]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# label_train = gt_label_wild\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# gt training, saplma\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostive: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(label_train\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m negative: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(label_train\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m best_acc, final_acc, (\n\u001b[0;32m---> 25\u001b[0m     clf, best_state, best_preds, preds, labels_val), losses_train \u001b[38;5;241m=\u001b[39m \u001b[43mget_linear_acc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_ret\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcosine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnonlinear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m clf\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m clf(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\n\u001b[1;32m     40\u001b[0m     embed_generated_test[:, layer, :])\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mcuda())\n",
      "File \u001b[0;32m/home/hallscope/linear_probe.py:292\u001b[0m, in \u001b[0;36mget_linear_acc\u001b[0;34m(ftrain, ltrain, ftest, ltest, n_cls, epochs, args, classifier, print_ret, normed, nonlinear, learning_rate, weight_decay, batch_size, cosine, lr_decay_epochs)\u001b[0m\n\u001b[1;32m    289\u001b[0m adjust_learning_rate(opt, optimizer, epoch)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m loss_train, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# eval for one epoch\u001b[39;00m\n\u001b[1;32m    295\u001b[0m loss, val_acc, preds, labels_out \u001b[38;5;241m=\u001b[39m validate(val_loader, classifier, criterion, print_freq\u001b[38;5;241m=\u001b[39mopt\u001b[38;5;241m.\u001b[39mprint_freq)\n",
      "File \u001b[0;32m/home/hallscope/linear_probe.py:164\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, classifier, criterion, optimizer, epoch, print_freq)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# acc1, acc5 = accuracy(output, labels, topk=(1, 5))\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# breakpoint()\u001b[39;00m\n\u001b[1;32m    162\u001b[0m correct \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msigmoid(output) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39meq(labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 164\u001b[0m \u001b[43mtop1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# SGD\u001b[39;00m\n\u001b[1;32m    167\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/home/hallscope/linear_probe.py:76\u001b[0m, in \u001b[0;36mAverageMeter.update\u001b[0;34m(self, val, n)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val \u001b[38;5;241m*\u001b[39m n\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, num=40)[1:-1]\n",
    "\n",
    "# graid search\n",
    "auroc_over_thres = []\n",
    "for thres_wild in tqdm(thresholds):\n",
    "    best_auroc = 0\n",
    "    best_accuracy=0\n",
    "    for layer in range(len(embed_generated_wild[0])):\n",
    "        thres_wild_score = np.sort(best_scores)[\n",
    "            int(len(best_scores) * thres_wild)]\n",
    "        true_wild = embed_generated_wild[:, layer,:][best_scores > thres_wild_score]\n",
    "        false_wild = embed_generated_wild[:, layer,:][best_scores <= thres_wild_score]\n",
    "\n",
    "        embed_train = np.concatenate([true_wild, false_wild], 0) # true label: 1; false label:0\n",
    "        label_train = np.concatenate([np.ones(len(true_wild)),\n",
    "                                      np.zeros(len(false_wild))], 0)\n",
    "\n",
    "        # gt training, saplma\n",
    "        # embed_train = embed_generated_wild[:,layer,:]\n",
    "        # label_train = gt_label_wild\n",
    "        # gt training, saplma\n",
    "        print(f'postive: {sum(label_train==1)} negative: {sum(label_train==0)}')\n",
    "\n",
    "        best_acc, final_acc, (\n",
    "            clf, best_state, best_preds, preds, labels_val), losses_train = get_linear_acc(\n",
    "            embed_train,\n",
    "            label_train,\n",
    "            embed_train,\n",
    "            label_train,\n",
    "            2, epochs=50,\n",
    "            print_ret=True,\n",
    "            batch_size=512,\n",
    "            cosine=True,\n",
    "            nonlinear=True,\n",
    "            learning_rate=0.05,\n",
    "            weight_decay=0.0003)\n",
    "\n",
    "        clf.eval()\n",
    "        output = clf(torch.from_numpy(\n",
    "            embed_generated_test[:, layer, :]).to(torch.float32).cuda())\n",
    "        pca_wild_score_binary_cls = torch.sigmoid(output)\n",
    "\n",
    "        pca_wild_score_binary_cls = pca_wild_score_binary_cls.cpu().data.numpy()\n",
    "\n",
    "        #pred_labels=(pca_wild_score_binary_cls >= 0.5).astype(int)\n",
    "        \n",
    "        #pred_acc=accuracy_score(gt_label_test,pred_labels)\n",
    "\n",
    "        if np.isnan(pca_wild_score_binary_cls).sum() > 0:\n",
    "            breakpoint()\n",
    "        measures = get_measures(pca_wild_score_binary_cls[gt_label_test == 1],\n",
    "                                pca_wild_score_binary_cls[gt_label_test == 0], plot=False)\n",
    "        \n",
    "        #auroc=roc_auc_score(gt_label_test,pca_wild_score_binary_cls)\n",
    "        #if auroc> best_auroc:\n",
    "        #    best_auroc=auroc\n",
    "        #    best_accuracy=pred_acc\n",
    "        #    best_layer=layer\n",
    "\n",
    "        #if pred_acc> best_accuracy:\n",
    "        #    best_accuracy=pred_acc\n",
    "        \n",
    "        \n",
    "        if measures[0] > best_auroc:\n",
    "            best_auroc = measures[0]\n",
    "            best_result = [100 * measures[0]]\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(gt_label_test,pca_wild_score_binary_cls)\n",
    "\n",
    "            # Calculate Youden's J statistic\n",
    "            youdens_j = tpr - fpr\n",
    "            # Find the index of the maximum J statistic \n",
    "            best_index = np.argmax(youdens_j) \n",
    "            best_threshold = thresholds[best_index]\n",
    "\n",
    "            pred_labels=(pca_wild_score_binary_cls >= best_threshold).astype(int)\n",
    "            best_accuracy=accuracy_score(gt_label_test,pred_labels)\n",
    "\n",
    "            best_layer = layer\n",
    "\n",
    "\n",
    "    print('thres: ', np.sort(best_scores)[\n",
    "            int(len(best_scores) * thres_wild)], 'best result: ',\n",
    "          best_result, 'best_layer: ', best_layer,'best_accuracy',best_accuracy,'linear best_threshold',best_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

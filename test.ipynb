{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/root/.local/share/jupyter/runtime/kernel-v3b4d1ef52916988031ca29bdc7e95bb54a7280dc5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from llm_utils.llm_loader import load_llm\n",
    "from utils.prompt import Prompter\n",
    "from llm_utils.llm_generation import LLMGeneration\n",
    "from utils.store_data import StoreData\n",
    "from utils.arguments import Arguments\n",
    "from dataset_loaders import load_data, ImageDataset\n",
    "from dataset_loaders.utils import data_sampler\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import display\n",
    "\n",
    "def main(args):\n",
    "    # Load dataset\n",
    "    prompter = Prompter(args.prompt, args.theme)\n",
    "    data = load_data(args.dataset, args)\n",
    "\n",
    "    if args.num_samples is not None:\n",
    "        data = data_sampler(\n",
    "            data, num_samples=args.num_samples, shuffle=args.shuffle)\n",
    "\n",
    "    image_dataset = ImageDataset(data, args.image_shape)\n",
    "    data_loader = DataLoader(\n",
    "        image_dataset, batch_size=args.batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # Load LLM\n",
    "    model, processor = load_llm(args.model_name, args.model_path)\n",
    "    llm_generation = LLMGeneration(model, processor)\n",
    "    store_data = StoreData(args.save_path)\n",
    "\n",
    "    # Generate responses and embeddings\n",
    "    for batch in tqdm(data_loader):\n",
    "        results = llm_generation.multi_generate(\n",
    "            batch['question'], batch['img'], args)\n",
    "\n",
    "        batch.update(results)\n",
    "        del batch['img']\n",
    "        '''\n",
    "        ins={\n",
    "        'img_path':the path of input image\n",
    "        'img':the image tensor (no stored)\n",
    "        'question': the question\n",
    "        'answers':[a list of example answers]\n",
    "        'most_likely':{\n",
    "            'response': the most likely response\n",
    "            'embedding': hidden_states\n",
    "        },\n",
    "        'responses':[other generation sequences],\n",
    "        '...': other dataset specific keys\n",
    "        }\n",
    "        '''\n",
    "        store_data.store(batch)\n",
    "\n",
    "\n",
    "arguments = Arguments(\n",
    "    '/home/hallscope/configs/ChartVQA/train.yaml')\n",
    "args = arguments.get_config()\n",
    "main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
